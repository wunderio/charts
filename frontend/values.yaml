# Configuration for the wunderio/silta-release subchart.
# see: https://github.com/wunderio/charts/blob/master/silta-release/values.yaml
silta-release:
  downscaler:
    enabled: true

# Main domain of the cluster.
# Subdomains of this domain will be created automatically for each environment.
clusterDomain: "silta.wdr.io"

# An optional human-readable label for the project, defaults to the repository name.
# This name is mainly used to create nice subdomains for each environment.
projectName: ""

# An optional human-readable label for the environment, defaults to the release name.
# We typically pass the branch name when we build dedicated environments per branch.
# This name is mainly used to create nice subdomains for each environment.
environmentName: ""

# The app label added to our Kubernetes resources.
app: frontend

# Domain names that will be mapped to this deployment.
# Example of exposing 2 additional domains for this deployment, each with its own certificate.
# exposeDomains:
#   example:
#     hostname: example.com
#   example2:
#     hostname: example1.com
#     # Reference to a key under `ingress`
#     ingress: gce
#     ssl:
#       enabled: true
#       issuer: letsencrypt-staging
#   example_www:
#     hostname: www.example.com
#   example_no_https:
#    hostname: insecure.example.com
#      ssl: 
#        enabled: false
exposeDomains: {}

exposeDomainsDefaults:
  ingress: default
  ssl:
    enabled: true
    issuer: letsencrypt

# Settings for default site provided by this deployment
ssl:
  # Enable HTTPS for this deployment
  enabled:  true
  # Possible issuers: letsencrypt-staging, letsencrypt, selfsigned, custom
  issuer: letsencrypt
  # Only when certificate type is custom
  # ca: ""
  # key: ""
  # crt: ""

ingress:
  default:
    type: traefik
    tls: true
    redirect-https: true
  gce:
    type: gce
    # The name of the reserved static IP address.
    # It is best to first reserve the IP address and then add it here.
    # staticIpAddressName: null

# Infrastructure related settings.
cluster: 
  type: gke
  vpcNative: false

# backendConfig customizations for main service
backendConfig:
  securityPolicy:
    name: "silta-ingress"

# Add prefixes to the generated per-branch domains, to be used for projects that need to respond
# on multiple domains.
# domainPrefixes: ['en', 'fi']
domainPrefixes: []

# These variables are build-specific and should be passed via the --set parameter.
nginx:
  image: 'wunderio/drupal-nginx:v0.1'

  replicas: 1

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 3
    metrics:
      - type: Resource
        resource:
          name: cpu
          targetAverageUtilization: 80

  # The Kubernetes resources for the nginx container.
  # These values are optimised for development environments.
  resources:
    requests:
      cpu: 1m
      memory: 10M

  loglevel: error 

  # Trust X-Forwarded-For from these hosts for getting external IP 
  realipfrom: 10.0.0.0/8

  # Add IP addresses that should be excluded from basicauth.
  # Note that the key is only used for documentation purposes.
  noauthips:
    gke-internal: 10.0.0.0/8
  
  # Set of values to enable and use http basic authentication
  # It is implemented only for very basic protection (like web crawlers)
  basicauth:
    enabled: true

    # Define username and password
    credentials:
      username: silta
      password: demo

  # Robots txt file blocked by default
  robotsTxt:
    allow: false
 
  # Security headers
  # includeSubdomains should be used whenever possible, but before enabling it needs to be made sure there are no subdomains not using https:
  hsts_include_subdomains: ""
  #hsts_include_subdomains: " includeSubDomains;"
  #content_security_policy: "upgrade-insecure-requests; default-src https: data: 'unsafe-inline' 'unsafe-eval'; frame-ancestors 'self'; base-uri 'self'; object-src 'self'; connect-src wss: https:"

  extra_headers: {}

  extra_conditions: ""
    # |
    # # need this for node OPTIONS requests to work while site has bauth
    # add_header Access-Control-Allow-Methods 'GET,OPTIONS,PUT,DELETE,POST' always;
    # add_header Access-Control-Allow-Credentials 'true' always;
    # add_header Access-Control-Allow-Origin '$http_origin' always;
    # add_header Access-Control-Allow-Headers 'Authorization,DNT,User-Agent,Keep-Alive,Content-Type,accept,origin,X-Requested-With,Access-Control-Allow-Origin' always;
    # if ($request_method = OPTIONS ) {
    #   return 204;
    # }

services: {}
  # node:
  #   image: 'you need to pass in a value for frontend.image to your helm chart'
  #   port: 3000
  #   env: {}
  #   # When you use exposedRoute, nginx proxies request to service:[/exposedRoute] path.
  #   # This means, you have to implement this path in your application.
  #   #exposedRoute: '/'
    
  #   # How many instances of the Frontend pod should be in our Kubernetes deployment.
  #   # A single pod (the default value) is good for development environments to minimise resource usage.
  #   # Multiple pods make sense for high availability.
  #   replicas: 1
  
  #   # Use storage mountpoints (defined in the mounts section) for this service.
  #   mounts:
  #     - files

  #   # Enable autoscaling using HorizontalPodAutoscaler
  #   # see: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
  #   autoscaling:
  #     enabled: false
  #     minReplicas: 1
  #     maxReplicas: 3
  #     metrics:
  #       - type: Resource
  #         resource:
  #           name: cpu
  #           targetAverageUtilization: 80

  #   resources: 
  #     requests:
  #       cpu: 200m
  #       memory: 128Mi
  #     limits:
  #       memory: 512Mi

  #  # Post-install hook.
  #  # This is run every time a new environment (most commonly a first deployment for a new branch) is created
  #  postinstall:
  #    command: |
  #      your command goes here

  #  # Post-upgrade hook.
  #  # This is run every time a new release is deployed
  #  postupgrade:
  #    command: |
  #      your command goes here

  #  cron:
  #    example:
  #      command: echo "hello world"
  #      schedule: "~ 1 * * *"
  #      parallelism: 1
  #      nodeSelector:
  #        cloud.google.com/gke-nodepool: static-ip
  #      # Optionally override service resource requests
  #      resources:
  #        requests:
  #          cpu: 500m

  #   nodeSelector:
  #     cloud.google.com/gke-nodepool: static-ip

# Configure the dynamically mounted volumes
mounts: {}
#  files:
#    enabled: true
#    storage: 1G
#    mountPath: /app/files
#    storageClassName: silta-shared
#    csiDriverName: csi-rclone
#    accessModes: ReadWriteMany

# Provide SSH access based on GitHub public keys and repository access. 
# Note: Shell only works when the base image wunderio/silta-node is used!
shell:
  enabled: false
  gitAuth:
    apiToken: ''
    # Project's git repository URL
    repositoryUrl: ''
    outsideCollaborators: true
    keyserver:
      # Defaults to https://keys.[clusterDomain]/api/1/git-ssh-keys
      url: ''
      username: ''
      password: ''
  mount:
    storageClassName: silta-shared
    csiDriverName: csi-rclone
    accessModes: ReadWriteMany

# Provide defaults for all services.
# Note that only keys used here can be overridden.
serviceDefaults:
  port: 3000
  resources:
    requests:
      cpu: 2m
      memory: 64Mi
    limits:
      memory: 256Mi
  # https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  strategy:
    type: RollingUpdate
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 3
    metrics:
      - type: Resource
        resource:
          name: cpu
          targetAverageUtilization: 80

# Override the default values of the MariaDB subchart.
# These settings are optimised for development environments.
# see: https://github.com/bitnami/charts/blob/master/bitnami/mariadb/values.yaml
mariadb:
  enabled: false
  replication:
    enabled: false
  db:
    name: frontend
    user: frontend
  master:
    persistence:
      # Database storage disk space allocation
      # Request assistance from ops team after changing this on existing deployment.
      size: 1G
    resources:
      requests:
        cpu: 25m
        memory: 384M
      limits:
        memory: 512M
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          preference:
            matchExpressions:
            - key: cloud.google.com/gke-nodepool
              operator: NotIn
              values:
              - static-ip

elasticsearch:
  enabled: false

  # The elasticsearch version to use.
  # It's a good idea to tag this in your silta.yml
  imageTag: 7.4.1

  replicas: 1
  minimumMasterNodes: 1
  maxUnavailable: 0
  clusterHealthCheckParams: 'wait_for_status=yellow&timeout=1s'

  # This value should be slightly less than 50% of the requested memory.
  esJavaOpts: -Xmx220m -Xms220m
  xpack:
    enabled: false
  volumeClaimTemplate:
    resources:
      requests:
        storage: 1Gi

  resources:
    requests:
      cpu: 200m
      memory: 512Mi
    limits:
      memory: 1Gi

  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      preference:
        matchExpressions:
        - key: cloud.google.com/gke-nodepool
          operator: NotIn
          values:
          - static-ip

rabbitmq:
  enabled: false

  # Use a low default to prevent unnecessary storage use.
  persistence:
    size: 1Gi
