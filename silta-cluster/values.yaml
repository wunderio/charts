# Main domain of the cluster.
clusterDomain: "silta.wdr.io"

ingress:
  # Options: traefik, gce, azure/application-gateway, etc.
  class: traefik
  redirect-https: true
  # Custom ingress annotations
  extraAnnotations: {}
  #  networking.gke.io/suppress-firewall-xpn-error: "true"

# Community ingress-nginx chart
# https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/
# https://github.com/kubernetes/ingress-nginx/blob/106e633655e7e5799ccf28d747b07d78833cd860/charts/ingress-nginx/values.yaml
ingress-nginx:
  enabled: false
  controller:
    # see autoscaling section below
    replicaCount: 1
    service:
      # loadBalancerIP: 1.2.3.4
      externalTrafficPolicy: Local
    # ingressClass: nginx
    ingressClassResource:
      default: false
      # name: nginx
    watchIngressWithoutClass: false
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 10
      targetCPUUtilizationPercentage: 80
      targetMemoryUtilizationPercentage: 80
    config:
      # Optional CDN configuration
      use-proxy-protocol: false
      use-forwarded-headers: true
      compute-full-forwarded-for: true
      # forwarded-for-header: "X-Forwarded-For"
      # proxy-real-ip-cidr: "1.1.1.1/32, 2.2.2.2/32"
      # Logging configuration
      disable-access-log: false
      disable-http-access-log: false
      disable-stream-access-log: false
      upstream-keepalive-requests: 16378
      upstream-keepalive-connections: 4096
      keep-alive-requests: 16378
      use-geoip: false
      use-gzip: false
      gzip-level: 1
      proxy-buffer-size: 8k
      # ModSecurity https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#enable-modsecurity
      # Off by default, enable using ingress annotations
      enable-modsecurity: false
      enable-owasp-modsecurity-crs: false
      # Required minimum configuration since SecRuleEngine is set to "DetectionOnly" by default
      # modsecurity-snippet: |
      #   SecRuleEngine On
      #   SecRequestBodyAccess On
    # Admission webhook is broken in GKE private clusters and requires additional configuration
    # so we disable it by default. Enable when possible.
    # Reference: https://github.com/kubernetes/ingress-nginx/issues/5401
    admissionWebhooks:
      enabled: false
    priorityClassName: "high-priority"
    resources:
      requests:
        cpu: "250m"
        memory: "256Mi"
      limits:
        cpu: 1
        memory: 1Gi
  # Splash page
  defaultBackend:
    enabled: true
    name: defaultbackend
    image:
      registry: wunderio
      image: silta-splash
      tag: "v1"
      readOnlyRootFilesystem: false

# Traefik 1.7 chart
# https://github.com/wunderio/charts/blob/master/legacy_traefik/values.yaml
# Note: Do not use this for new clusters, this is a default LB due to legacy setups. Use ingress-nginx instead.
traefik:
  enabled: true
  externalTrafficPolicy: Local
  priorityClassName: "high-priority"
  image: wunderio/silta-traefik
  imageTag: 1.7.34-mod
  rbac:
    enabled: true
  replicas: 2
  ssl:
    enabled: true
    enforced: false
    generateTLS: true
    defaultCN: "*.silta.wdr.io"
    # Values from Mozilla SSL Configuration Generator
    # https://ssl-config.mozilla.org/#server=traefik&server-version=1.7.20&config=intermediate
    tlsMinVersion: "VersionTLS12"
    cipherSuites:
      - TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384
      - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
      - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256
      - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
      - TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305
      - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
  dashboard:
    enabled: false
    serviceType: "NodePort"
    #auth:
    #  basic: 
    #    # htpasswd -nb someuser somepass
    #    someuser: htpass_hash
  resources:
    requests:
      cpu: 50m
      memory: 128M
    limits:
      memory: 512M

ssl:
  enabled: true
  email: admin@example.com
  # Available issuers: letsencrypt-staging, letsencrypt, selfsigned, custom
  issuer: letsencrypt
  # Used when certificate issuer is "custom"
  # ca: ""
  # key: ""
  # crt: ""

# Community ingress-nginx chart
# https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/
# https://github.com/kubernetes/ingress-nginx/blob/106e633655e7e5799ccf28d747b07d78833cd860/charts/ingress-nginx/values.yaml
# This is a duplicate of the ingress-nginx chart, but with a different name, meant to immitate traefik ingress class and replace it.
nginx-traefik:
  enabled: false
  controller:
    # see autoscaling section below
    replicaCount: 1
    service:
      # loadBalancerIP: 1.2.3.4
      externalTrafficPolicy: Local
    ingressClass: traefik
    ingressClassResource:
      default: true
      name: traefik
    watchIngressWithoutClass: false
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 10
      targetCPUUtilizationPercentage: 80
      targetMemoryUtilizationPercentage: 80
    # https://github.com/kubernetes/ingress-nginx/issues/6928
    minReadySeconds: 10
    # https://github.com/kubernetes/ingress-nginx/issues/6928
    extraArgs:
      shutdown-grace-period: 30
    config:
      # Optional CDN configuration
      use-proxy-protocol: false
      use-forwarded-headers: true
      compute-full-forwarded-for: true
      # forwarded-for-header: "X-Forwarded-For"
      # proxy-real-ip-cidr: "1.1.1.1/32, 2.2.2.2/32"
      # Logging configuration
      disable-access-log: false
      disable-http-access-log: false
      disable-stream-access-log: false
      upstream-keepalive-requests: 16378
      upstream-keepalive-connections: 4096
      keep-alive-requests: 16378
      use-geoip: false
      use-gzip: false
      gzip-level: 1
      proxy-buffer-size: 8k
      # default is too low (1m)
      proxy-body-size: 0
      # Extends client timeouts
      client-header-timeout: 120
      client-body-timeout: 120
      proxy-read-timeout: 120
      # Tackles https://github.com/kubernetes/ingress-nginx/issues/8488
      lua-shared-dicts: "certificate_data: 100"
      # Tackles https://github.com/kubernetes/ingress-nginx/issues/5030
      variables-hash-bucket-size: "256"
      variables-hash-max-size: "2048"
      # Different error codes to distinguish rate limit states
      limit-req-status-code: 420
      limit-conn-status-code:  529
      # ModSecurity https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#enable-modsecurity
      # Off by default, enable using ingress annotations
      enable-modsecurity: false
      enable-owasp-modsecurity-crs: false
      # Required minimum configuration since SecRuleEngine is set to "DetectionOnly" by default
      # modsecurity-snippet: |
      #   SecRuleEngine On
      #   SecRequestBodyAccess On
    # Admission webhook is broken in GKE private clusters and requires additional configuration
    # so we disable it by default. Enable when possible.
    # Reference: https://github.com/kubernetes/ingress-nginx/issues/5401
    admissionWebhooks:
      enabled: false
    priorityClassName: "high-priority"
    resources:
      requests:
        cpu: "250m"
        memory: "256Mi"
      limits:
        cpu: 1
        memory: 1Gi
    metrics:
      enabled: false
      service:
        annotations:
          prometheus.io/scrape: "true"
          prometheus.io/port: "10254"
  # Splash page
  defaultBackend:
    enabled: true
    name: defaultbackend
    image:
      registry: wunderio
      image: silta-splash
      tag: "v1"
      readOnlyRootFilesystem: false
  
# https://github.com/wunderio/charts/blob/master/csi-rclone/values.yaml
csi-rclone:
  enabled: false
  priorityClassName: "high-priority"
  # Use silta cluster gke credentials. This is used when remote type is set to "google cloud storage".
  useGkeAuth: false
  # Do not allow csi-rclone subchart to install default secret because we want to generate it ourselves.
  defaultParams: false
  storageClass:
    name: "silta-shared"
    pathPattern: "${.PVC.namespace}/${.PVC.annotations.storage.silta/storage-path}"
  # params:
  #   remote: "s3"
  #   remotePath: "projectname"
    
  #   # Minio as S3 provider
  #   s3-provider: "Minio"
  #   s3-endpoint: "http://silta-cluster-minio:9000"
  #   # Default credentials of minio chart https://github.com/minio/charts/blob/master/minio/values.yaml
  #   s3-access-key-id: "YOURACCESSKEY"
  #   s3-secret-access-key: "YOURSECRETKEY"

minio:
  enabled: false

# Splash page.
splash:
  resources:
    requests:
      cpu: 10m
      memory: 10M


# SSH Jumphost settings
gitAuth:
  enabled: true
  port: 22
  keyserver:
    enabled: true
    url: ''
    username: ''
    password: ''
  authorizedKeys: []
  scope: ''
  outsideCollaborators: true
  allowedIps: []
  replicas: 1
  persistence:
    # storageClassName: silta-shared
    accessMode: ReadWriteOnce
    size: 50M
  # Kubernetes resource allocation.
  resources: {}
  # The static IP to be used for the exposed endpoint.
  loadBalancerIP: null
  externalTrafficPolicy: Local

# SSH keyserver (keys.[clusterDomain])
sshKeyServer:
  enabled: true
  gitauthApiToken: ''
  apiUsername: ''
  apiPassword: ''
  replicas: 1
  resources: {}

# Deployment remover
deploymentRemover:
  enabled: true
  replicas: 2
  image: wunderio/silta-deployment-remover
  imageTag: v1
  # Github webhooks secret
  webhooksSecret: ''
  # Debugging mode. Does not remove resources when set to false; see pod logs when enabled.
  debug: false
  # Kubernetes resource allocation.
  resources: {}

# GKE Cluster settings
gke:
  keyJSON: ''
  projectName: '' 
  computeZone: ''
  clusterName: ''

# Percona XtraDB Cluster for replicated database support
# https://github.com/percona/percona-helm-charts/blob/859cbbb5f54ba30df12e758e10e3f941bf4cc956/charts/pxc-operator/values.yaml
pxc-operator:
  enabled: false
  watchAllNamespaces: true

# Cert manager moved to subchart, see installation instructions.
# Installs v0.10.1 when explicitly enabled, but it 
# does not work on kubernetes 1.20+
cert-manager:
  enabled: false

silta-downscaler:
  enabled: false
  image: wunderio/silta-downscaler
  imageTag: "v1"
  # A cron schedule to determine when applications will be downscaled.
  schedule: "0 4,18 * * *"
  # How long should releases be kept by default.
  defaultMinAge: 1d
  # How long should releases with matching names be kept. The last matching rule takes effect.
  releaseMinAge:
    "^production": 10y
    "^(master|main|stage|staging)": 8w
    "^(dev|develop|development)": 4w 
    "^dependabot": 1h
  resources:
    requests:
      cpu: 10m
      memory: 50Mi
  proxy:
    image: wunderio/silta-downscaler
    imageTag: v1-proxy

# https://github.com/wunderio/charts/blob/master/silta-proxy
silta-proxy:
  enabled: false

instana-agent:
  enabled: false

nfs-subdir-external-provisioner:
  enabled: false
  nfs:
    path: /main_share
  storageClass:
    name: nfs-shared
    pathPattern: "${.PVC.namespace}/${.PVC.annotations.storage.silta/storage-path}"
    onDelete: delete
  image:
    repository: docker.io/wunderio/nfs-subdir-external-provisioner
    tag: v5.0

# Uses nfs-subdir-external-provisioner. 
# see: https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner/blob/master/charts/nfs-subdir-external-provisioner/values.yaml
silta-shared-nfs:
  enabled: false
  nfs:
    server: silta-cluster-nfs-server.silta-cluster.svc.cluster.local
    path: /
  storageClass:
    name: silta-shared-nfs
    # Set volume bindinng mode - Immediate or WaitForFirstConsumer
    volumeBindingMode: WaitForFirstConsumer
    pathPattern: "${.PVC.namespace}/${.PVC.annotations.storage.silta/storage-path}"
    onDelete: delete
  image:
    repository: docker.io/wunderio/nfs-subdir-external-provisioner
    tag: v5.0

nfs-server:
  enabled: false
  image: k8s.gcr.io/volume-nfs
  imageTag: 0.8
  imagePullPolicy: IfNotPresent
  # Kubernetes resource assignments.
  resources:
    requests:
      cpu: 10m
      memory: 256M
    limits:
      cpu: 100m
      memory: 512M
  persistence:    
    rclone:
      enabled: false
      image: rclone/rclone:1.63
      # bucket
      remotePath: ""
      # rclone config file content (except [remote] header)
      # https://rclone.org/docs/#config-config-file
      backend_config: |
        # type = google cloud storage
        # project_number = test-project-123
        # service_account_credentials = { escaped json, convert newlines to \n }
        # location = region
        # links = true
        # gcs_directory_markers = true
        # bucket_policy_only = true
      # rclone mount commmand flags
      # https://rclone.org/commands/rclone_mount/
      params:
        cache-chunk-clean-interval: 15m
        dir-cache-time: 5s
        vfs-cache-mode: writes
        cache-info-age: 72h
        allow-non-empty: true
        allow-other: true
      resources:
        requests:
          cpu: 10m
          memory: 128M
    data:
      size: 10Gi
      # storageClassName: standard
      # Provide driver name if using CSI driver backed storage class (optional).
      # csiDriverName: csi-driver-name
      accessModes:
        - ReadWriteOnce

# This should be replaced when an actual sidecar implementation is available. 
# https://github.com/kubernetes/enhancements/issues/753#issuecomment-713471597
k8sControllerSidecars:
  enabled: true
  replicaCount: 1
  image:
    repository: wunderio/silta-k8s-controller-sidecars
    tag: latest
    pullPolicy: IfNotPresent
  resources:
    requests:
     cpu: 100m
     memory: 128Mi
    limits:
      cpu: "200m"
      memory: "256Mi"

# https://github.com/twuni/docker-registry.helm
docker-registry:
  enabled: false
  image:
    repository: distribution/distribution  
    tag: 2.8.1
  service:
    port: 80
  persistence:
    enabled: true
    size: 10Gi
  resources:  
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 100m
      memory: 192Mi
  secrets:
    # generate user:passwordhash using "htpasswd -Bbn user password" and keep it safe!
    # Note: allows multiple sets of credentials, use one per line
    htpasswd: |
      broken:auth

daemonset:
  enabled: true
  # Required by elasticsearch
  # https://www.elastic.co/guide/en/cloud-on-k8s/master/k8s-virtual-memory.html#k8s_using_a_daemonset_to_set_virtual_memory
  maxMapCountSetter:
    enabled: true
