# Main domain of the cluster.
# Subdomains of this domain will be created automatically for each environment.
clusterDomain: "silta.wdr.io"

# An optional human-readable label for the project, defaults to the repository name.
# This name is mainly used to create nice subdomains for each environment.
projectName: ""

# An optional human-readable label for the environment, defaults to the release name.
# We typically pass the branch name when we build dedicated environments per branch.
# This name is mainly used to create nice subdomains for each environment.
environmentName: ""

# The app label added to our Kubernetes resources.
app: frontend

# Domain names that will be mapped to this deployment.
# Example of exposing 2 additional domains for this deployment, each with its own certificate
# Note that the key is only used for documentation purposes.
# exposeDomains:
#   - hostname: example.com
#   - hostname: www.example.com
#   -   ssl:
#   -     enabled: true
#   -     issuer: letsencrypt-staging
exposeDomains: {}

ssl:
  # Enable HTTPS for this deployment
  enabled:  true
  # Possible issuers: letsencrypt-staging, letsencrypt, selfsigned, custom
  issuer: letsencrypt
  # Only when certificate type is custom
  # ca: ""
  # key: ""
  # crt: ""

# These variables are build-specific and should be passed via the --set parameter.
nginx:
  image: 'wunderio/drupal-nginx:v0.1'

  replicas: 1

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 3
    metrics:
      - type: Resource
        resource:
          name: cpu
          targetAverageUtilization: 80

  # The Kubernetes resources for the nginx container.
  # These values are optimised for development environments.
  resources:
    requests:
      cpu: 1m
      memory: 10M

  loglevel: error 

  # Trust X-Forwarded-For from these hosts for getting external IP 
  realipfrom: 10.0.0.0/8

  # Add IP addresses that should be excluded from basicauth.
  # Note that the key is only used for documentation purposes.
  noauthips:
    gke-internal: 10.0.0.0/8
  
  # Set of values to enable and use http basic authentication
  # It is implemented only for very basic protection (like web crawlers)
  basicauth:
    enabled: true

    # Define username and password
    credentials:
      username: silta
      password: demo

  # Robots txt file blocked by default
  robotsTxt:
    allow: false

  extra_headers: {}

  extra_conditions: ""
    # |
    # # need this for node OPTIONS requests to work while site has bauth
    # add_header 'Access-Control-Allow-Methods' 'GET,OPTIONS,PUT,DELETE,POST' always;
    # add_header 'Access-Control-Allow-Credentials' 'true' always;
    # add_header 'Access-Control-Allow-Origin' '$http_origin' always;
    # add_header 'Access-Control-Allow-Headers' 'Authorization,DNT,User-Agent,Keep-Alive,Content-Type,accept,origin,X-Requested-With,Access-Control-Allow-Origin' always;
    # if ($request_method = OPTIONS ) {
    #   return 204;
    # }

services: {}
  # node:
  #   image: 'you need to pass in a value for frontend.image to your helm chart'
  #   port: 3000
  #   env: {}
  #   # When you use exposedRoute, nginx proxies request to service:[/exposedRoute] path.
  #   # This means, you have to implement this path in your application.
  #   #exposedRoute: '/'
    
  #   # How many instances of the Frontend pod should be in our Kubernetes deployment.
  #   # A single pod (the default value) is good for development environments to minimise resource usage.
  #   # Multiple pods make sense for high availability.
  #   replicas: 1

  #   # Enable autoscaling using HorizontalPodAutoscaler
  #   # see: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
  #   autoscaling:
  #     enabled: false
  #     minReplicas: 1
  #     maxReplicas: 3
  #     metrics:
  #       - type: Resource
  #         resource:
  #           name: cpu
  #           targetAverageUtilization: 80

  #   resources: 
  #     requests:
  #       cpu: 200m
  #       memory: 128Mi
  #     limits:
  #       memory: 512Mi

  #   cron:
  #     example:
  #       command: echo "hello world"
  #       schedule: "~ 1 * * *"
  #       parallelism: 1

  #   nodeSelector:
  #     cloud.google.com/gke-nodepool: static-ip

# Configure the dynamically mounted volumes
mounts: {}
#  files:
#    enabled: true
#    storage: 1G
#    mountPath: /app/files
#    storageClassName: silta-shared
#    csiDriverName: csi-rclone

# Provide SSH access based on GitHub public keys and repository access.
shell:
  enabled: false
  gitAuth:
    apiToken: ''
    repositoryURL: ''
  mount:
    storageClassName: silta-shared
    csiDriverName: csi-rclone

# Provide defaults for all services.
# Note that only keys used here can be overridden.
serviceDefaults:
  port: 3000
  resources:
    requests:
      cpu: 2m
      memory: 64Mi
    limits:
      memory: 256Mi
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 3
    metrics:
      - type: Resource
        resource:
          name: cpu
          targetAverageUtilization: 80

elasticsearch:
  enabled: false

  # The elasticsearch version to use.
  # It's a good idea to tag this in your silta.yml
  imageTag: 7.4.1

  replicas: 1
  minimumMasterNodes: 1
  maxUnavailable: 0
  clusterHealthCheckParams: 'wait_for_status=yellow&timeout=1s'

  # This value should be slightly less than 50% of the requested memory.
  esJavaOpts: -Xmx220m -Xms220m
  xpack:
    enabled: false
  volumeClaimTemplate:
    resources:
      requests:
        storage: 1Gi

  resources:
    requests:
      cpu: 200m
      memory: 512Mi
    limits:
      memory: 1Gi

  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      preference:
        matchExpressions:
        - key: cloud.google.com/gke-nodepool
          operator: NotIn
          values:
          - static-ip

rabbitmq:
  enabled: false

  # Use a low default to prevent unnecessary storage use.
  persistence:
    size: 1Gi
